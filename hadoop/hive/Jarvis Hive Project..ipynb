{
  "metadata": {
    "name": "Jarvis Hive Project",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Nariman Alimuradov\nJarvis Hive Project Notebook"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Load Data to HDFS"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_gs;\n\nCREATE EXTERNAL TABLE wdi_gs\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED \nFIELDS TERMINATED BY \u0027,\u0027 \nLINES TERMINATED BY \u0027\\n\u0027\nLOCATION \u0027gs://jarvis_data_eng_nariman/datasets/wdi_2016\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_csv_text;\r\n\r\nCREATE EXTERNAL TABLE wdi_csv_text\r\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\r\nROW FORMAT DELIMITED \r\nFIELDS TERMINATED BY \u0027,\u0027 \r\nLINES TERMINATED BY \u0027\\n\u0027\r\nLOCATION \u0027hdfs:///user/nariman/hive/wdi/wdi_csv_text\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls hdfs:///user/nariman/hive/wdi/wdi_csv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh \nhdfs dfs -cat  hdfs:///user/nariman/hive/wdi/wdi_csv_text/000000_0| head -100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "INSERT OVERWRITE TABLE wdi_csv_text\r\nSELECT * FROM wdi_gs"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- Cache Observations\n-- Below is a query that gets the number of entries in the wdi_csv_text table\nSELECT count(countryName) FROM wdi_csv_text;\n\n-- After clearing master and worker caches, fetching the size of the table took: 35.948 seconds\n-- On a subsequent run, it took: 15.472 seconds\n-- This is because a run will store data into the cache, so that subsequent runs don\u0027t take as long. It is the reason caches even exist.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- Hive vs Bash Performance Results\n-- The below cell contains a Bash approach to counting the rows in the data. The Bash solution took approximately 21 seconds when cache is cleared (compare this to 36 seconds in Hive).\n-- This is likely because of the overhead required by Hive to set up the worker nodes and push the computation through the whole pipeline whereas the Bash solution simply reads the file without any of that extra work."
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\ncd ~\nhdfs  dfs -get  hdfs:///user/nariman/hive/wdi/wdi_csv_text .\n\ncd wdi_csv_text\ndu -ch .\n\necho 3 | sudo tee /proc/sys/vm/drop_caches\n\ndate +%s \u0026\u0026 cat * | wc \u0026\u0026 date +%s"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Parsing Issue"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT *\r\nFROM wdi_csv_text\r\nLIMIT 100"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- We are separating by commas and because there are commas within entries, it is splitting it into separate lines. Had we not selected for distinct values, we would get lots of \u0027% of export of goods\u0027 lines as they appear in every entry."
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_gs_debug;\n\nCREATE EXTERNAL TABLE wdi_gs_debug\n(line STRING)\nLOCATION \u0027gs://jarvis_data_eng_nariman/datasets/wdi_2016\u0027\nTBLPROPERTIES (\"skip.header.line.count\"\u003d\"1\");"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT line\nFROM wdi_gs_debug\nLIMIT 100"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_opencsv_gs;\n\nCREATE EXTERNAL TABLE wdi_opencsv_gs\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \u0027gs://jarvis_data_eng_nariman/datasets/wdi_2016\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_opencsv_text;\n\nCREATE EXTERNAL TABLE wdi_opencsv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \u0027hdfs:///user/nariman/hive/wdi/wdi_opencsv_text\u0027;"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "INSERT OVERWRITE TABLE wdi_opencsv_text\nSELECT * FROM wdi_opencsv_gs"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT *\nFROM wdi_opencsv_text\nLIMIT 10"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT distinct(indicatorcode)\r\nFROM wdi_opencsv_text\r\nORDER BY indicatorcode\r\nLIMIT 20"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) \nFROM wdi_opencsv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName)\nFROM wdi_csv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- 1 min 26 sec for wdi_opencsv_text, 12 sec for wdi_csv_text\n-- It\u0027s slower because we are using the SerDe that must go through each line and change it to fit our needs. This adds a massive amount of overhead."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "OpenCSVSerde Limitation"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP VIEW IF EXISTS wdi_opencsv_text_view;\n\nCREATE VIEW wdi_opencsv_text_view\nAS\nSELECT year, countryName, countryCode, indicatorName, indicatorCode, CAST(indicatorValue AS FLOAT)\nFROM wdi_opencsv_text;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "2015 Canada GDP Growth HQL"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT indicatorValue AS GDP_growth_value, year, countryName\nFROM wdi_opencsv_text_view\nWHERE indicatorName LIKE \"%GDP growth%\" AND year \u003d 2015 AND countryName \u003d \"Canada\""
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- Searching through each row to find matches will take linear time, and this can be problematic when dealing with such large datasets. \n-- We can speed this up by potentially using a columnar file, or by partitioning and bucketing our view table. This will allow us to look through less data to get our results."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Hive Partitions"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SET hive.exec.dynamic.partition.mode \u003d nonstrict;\nDROP TABLE IF EXISTS wdi_opencsv_text_partitions;\n\nCREATE EXTERNAL TABLE wdi_opencsv_text_partitions\n(countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nPARTITIONED BY (year INTEGER)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \u0027hdfs:///user/nariman/hive/wdi/wdi_opencsv_text_partitions\u0027;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "INSERT OVERWRITE TABLE wdi_opencsv_text_partitions PARTITION (year)\r\nSELECT countryName, countryCode, indicatorName, indicatorCode, indicatorValue, year\r\nFROM wdi_opencsv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT indicatorValue AS GDP_growth_value, year, countryName\nFROM wdi_opencsv_text_partitions\nWHERE indicatorName LIKE \"%GDP growth%\" AND year \u003d 2015 AND countryName \u003d \"Canada\""
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls /user/nariman/hive/wdi/wdi_opencsv_text_partitions"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- 57 partitions were created.\n-- Our previous execution time was 1 minute 23 seconds, but partitioning the data brought that down to 13 seconds.\n-- This is because we no longer need to check all our data, but instead we can check the partition(s) that contain what we need."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Columnar File Optimization"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_csv_parquet;\n\nCREATE EXTERNAL TABLE wdi_csv_parquet\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nSTORED AS PARQUET\nLOCATION \u0027hdfs:///user/nariman/hive/wdi/wdi_csv_parquet\u0027;"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "INSERT OVERWRITE TABLE wdi_csv_parquet\nSELECT * FROM wdi_opencsv_gs"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -du -h /user/nariman/hive/wdi/wdi_csv_parquet\nhdfs dfs -du -h /user/nariman/hive/wdi/wdi_opencsv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName)\nFROM wdi_csv_parquet;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName)\nFROM wdi_opencsv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT indicatorValue AS GDP_growth_value, year, countryName\nFROM wdi_csv_parquet\nWHERE indicatorName LIKE \"%GDP growth%\" AND year \u003d 2015 AND countryName \u003d \"Canada\";"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT indicatorValue AS GDP_growth_value, year, countryName\nFROM wdi_opencsv_text\nWHERE indicatorName LIKE \"%GDP growth%\" AND year \u003d 2015 AND countryName \u003d \"Canada\";"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- The wdi_csv_parquet table is approximately 20 times smaller than the wdi_opencsv_text table, due to the columnar format which groups columns beside each other.\n-- For both the count and GDP queries, our columnar file optimization reduced runtime by approximately 5 times. Since the data is partitioned by column, our queries that require us to find certain values from each row become faster.\n-- This is because we no longer need to check each entry in the table, rather we check the singular column that contains our information."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Highest GDP Growth"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT distinct table1.indicatorValue AS GDP_growth_value, table1.countryName, table1.year\nFROM wdi_csv_parquet table1\nINNER JOIN (\n    SELECT max(indicatorValue) AS GDP_growth_value, countryName\n    FROM wdi_csv_parquet\n    WHERE indicatorCode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027\n    GROUP BY countryName\n    ) table2\nON table1.indicatorValue \u003d table2.GDP_growth_value AND table1.countryName \u003d table2.countryName\nORDER BY GDP_growth_value DESC;"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT distinct table1.indicatorValue AS GDP_growth_value, table1.countryName, table1.year\nFROM wdi_csv_parquet table1\nINNER JOIN (\n    SELECT max(indicatorValue) AS GDP_growth_value, countryName\n    FROM wdi_csv_parquet\n    WHERE indicatorCode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027\n    GROUP BY countryName\n    ) table2\nON table1.indicatorValue \u003d table2.GDP_growth_value AND table1.countryName \u003d table2.countryName\nORDER BY GDP_growth_value DESC;"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- Execution time for Tez: 1 minute 29 seconds\n-- Execution time for Spark: 1 minute 5 seconds"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Sort GDP by Country and Year"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT countryName, year, indicatorCode, indicatorValue\nFROM wdi_csv_parquet\nWHERE indicatorCode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027\nSORT BY countryName, year;"
    }
  ]
}